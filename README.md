# Retrieval-Augmented Generation (RAG) Pipeline with LangChain, Google Gemini, and GROQ Inference
<br/>
## ğŸš€ Overview
<br/>
RAG combines retrieval from external knowledge sources with generative models, allowing the system to provide accurate and contextually rich answers to user queries.
<br/>
---
<br/>
#  ğŸ› ï¸ Featuresc
<br/>
![Screenshot 2025-01-12 235516](https://github.com/user-attachments/assets/474fafa1-dcb0-41d5-ace7-49e6cbea5986)


#  ğŸ› ï¸ Features
<br/>
<br/>
- **Customizable Workflow:** Modify prompt templates and retrieval strategies.
<br/>
- **Scalable Architecture:** Optimized for high-performance inference with GROQ hardware.
<br/>
- **Pretrained LLMs:** Powered by Google Gemini for accurate and fluent generation.
<br/>
- **Knowledge Retrieval:** Uses vector stores for retrieving contextually relevant documents.

<br/>
# Accuracy
<br/>
![image](https://github.com/user-attachments/assets/3d8e28be-3cca-4d01-bec0-4199cfbf77b4)


