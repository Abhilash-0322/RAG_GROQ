# Retrieval-Augmented Generation (RAG) Pipeline with LangChain, Google Gemini, and GROQ Inference
<br/>
## üöÄ Overview
<br/>
RAG combines retrieval from external knowledge sources with generative models, allowing the system to provide accurate and contextually rich answers to user queries.
<br/>
---
<br/>
# Flowchart
<br/>
![image](https://github.com/user-attachments/assets/74e8fb13-efc4-4b8b-9931-aa09d608de40)
<br/>

#  üõ†Ô∏è Features
<br/>
<br/>
- **Customizable Workflow:** Modify prompt templates and retrieval strategies.
<br/>
- **Scalable Architecture:** Optimized for high-performance inference with GROQ hardware.
<br/>
- **Pretrained LLMs:** Powered by Google Gemini for accurate and fluent generation.
<br/>
- **Knowledge Retrieval:** Uses vector stores for retrieving contextually relevant documents.

<br/>
# Accuracy
<br/>
![image](https://github.com/user-attachments/assets/3d8e28be-3cca-4d01-bec0-4199cfbf77b4)


