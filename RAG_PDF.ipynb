{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\codespace\\ML\\NYD\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Doc Assistant Using Llama3\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Load the GROQ and OpenAI API keys\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "print(\"RAG Doc Assistant Using Llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=groq_api_key,\n",
    "    model_name=\"Llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     you are a bhagwatgita chatbot , you will have to provide the responses based on the bhagwatgita document provided by the user\n",
    "#     and the question asked about it you have to provide responses mostly based on the document not in general.\n",
    "#     <context>\n",
    "#     {context}\n",
    "#     <context>\n",
    "#     Questions:{input}\n",
    "#     \"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a document assistant that helps users find information in a context.\n",
    "    Please provide the most accurate response based on the context and inputs.\n",
    "    Only give information that is in the context, not general information.\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Questions: {input}\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the uploaded PDF and create vector embeddings\n",
    "def vector_embedding(file_path):\n",
    "    print(\"Processing file:\", file_path)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    loader = PyPDFLoader(file_path)  # Load PDF\n",
    "    docs = loader.load()  # Document Loading\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)  # Chunk Creation\n",
    "    final_documents = text_splitter.split_documents(docs[:20])  # Splitting\n",
    "    vectors = FAISS.from_documents(final_documents, embeddings)  # Vector embeddings\n",
    "    print(\"Vector Store DB is ready.\")\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\abhil\\Downloads\\NYDProgess.pdf\n",
      "Vector Store DB is ready.\n",
      "Response time: 0.015625\n",
      "Answer: Based on the context, the progress report is for the NYD Hackathon Week 1. Here is the summary:\n",
      "\n",
      "* Setuped Web Search Retrieval on Microsoft Azure\n",
      "* Implemented the RAG functionality as intended properly\n",
      "* Setuped the base LLMs in local environments\n",
      "* Tried to fine-tune Legacy Model GPT2, but it did not meet expectations\n",
      "* Planned to use different models, such as llama3, Gemma2, Gemini API, and Cohere API\n",
      "* Used llama3 with ollama locally for the langchain pipeline building\n",
      "\n",
      "No major achievements or milestones mentioned, but the report highlights the steps taken to set up the necessary infrastructure and explore different models for the task at hand.\n",
      "\n",
      "Document Similarity Search Results:\n",
      "Chunk 1:\n",
      " Tried Out Various RAG templates for the starƟng.  \n",
      " \n",
      " \n",
      " Setuped Web Search Retrival On MicrosoŌ Azure. \n",
      " \n",
      " \n",
      " \n",
      " Working  On ImplemenƟng The RAG funcƟonality As Intended Properly. \n",
      "\n",
      "--------------------------------\n",
      "Chunk 2:\n",
      "NYD Hackathon Week 1 Progess Report \n",
      " \n",
      " \n",
      " Setuped The Base LLMs in Local Environments.  \n",
      " \n",
      " Tried to Fine Tune Legacy Model GPT2 for aligning with the outcome the training loss \n",
      "reduced to around 0.2 but comes out to that GPT2 is not up to Mark With Vocabulary and \n",
      "cohesion. \n",
      " \n",
      " \n",
      " \n",
      " Then We Planned Use Diﬀerent Models Other the GPT2 the we went for llama3 and \n",
      "Gemma2 and For Other API AlternaƟves Gemini API and Cohere API. \n",
      " \n",
      " \n",
      " \n",
      " Then Used llama3 with ollama locally for the langchain pipeline building .\n",
      "--------------------------------\n",
      "Response time: 0.0\n",
      "Answer: Based on the provided context, the progress report for the NYD Hackathon Week 1 is as follows:\n",
      "\n",
      "* Setuped Web Search Retrieval on Microsoft Azure.\n",
      "* Set up the base LLMs in local environments.\n",
      "* Implemented the RAG functionality as intended properly.\n",
      "* Tried to fine-tune the Legacy Model GPT2, but it was not up to mark in terms of vocabulary and cohesion.\n",
      "* Planned to use different models (llama3 and Gemma2) and API alternatives (Gemini API and Cohere API).\n",
      "* Used llama3 with ollama locally for the langchain pipeline building.\n",
      "\n",
      "Overall, it appears that some progress has been made in setting up the infrastructure and experimenting with different models, but there are still some challenges to be addressed, particularly with the vocabulary and cohesion of the GPT2 model.\n",
      "\n",
      "Document Similarity Search Results:\n",
      "Chunk 1:\n",
      " Tried Out Various RAG templates for the starƟng.  \n",
      " \n",
      " \n",
      " Setuped Web Search Retrival On MicrosoŌ Azure. \n",
      " \n",
      " \n",
      " \n",
      " Working  On ImplemenƟng The RAG funcƟonality As Intended Properly. \n",
      "\n",
      "--------------------------------\n",
      "Chunk 2:\n",
      "NYD Hackathon Week 1 Progess Report \n",
      " \n",
      " \n",
      " Setuped The Base LLMs in Local Environments.  \n",
      " \n",
      " Tried to Fine Tune Legacy Model GPT2 for aligning with the outcome the training loss \n",
      "reduced to around 0.2 but comes out to that GPT2 is not up to Mark With Vocabulary and \n",
      "cohesion. \n",
      " \n",
      " \n",
      " \n",
      " Then We Planned Use Diﬀerent Models Other the GPT2 the we went for llama3 and \n",
      "Gemma2 and For Other API AlternaƟves Gemini API and Cohere API. \n",
      " \n",
      " \n",
      " \n",
      " Then Used llama3 with ollama locally for the langchain pipeline building .\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input the PDF file path\n",
    "file_path = input(\"Enter the path to the PDF file: \").strip()\n",
    "if not os.path.isfile(file_path):\n",
    "    print(\"File not found. Please provide a valid file path.\")\n",
    "else:\n",
    "    vectors = vector_embedding(file_path)\n",
    "\n",
    "    # Start the question loop\n",
    "    while True:\n",
    "        question = input(\"Enter your question (or type 'exit' to quit): \").strip()\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Create the retrieval chain and get the response\n",
    "        document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "        retriever = vectors.as_retriever()\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        \n",
    "        start = time.process_time()\n",
    "        response = retrieval_chain.invoke({'input': question})\n",
    "        print(\"Response time:\", time.process_time() - start)\n",
    "        print(\"Answer:\", response['answer'])\n",
    "        \n",
    "        # Display similar document chunks\n",
    "        print(\"\\nDocument Similarity Search Results:\")\n",
    "        for i, doc in enumerate(response[\"context\"]):\n",
    "            print(f\"Chunk {i + 1}:\\n{doc.page_content}\")\n",
    "            print(\"--------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
